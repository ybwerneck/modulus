{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28391227-7b25-4080-a670-584a1d421101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-26T03:21:06.721450Z",
     "iopub.status.busy": "2023-01-26T03:21:06.720615Z",
     "iopub.status.idle": "2023-01-26T03:21:06.733790Z",
     "shell.execute_reply": "2023-01-26T03:21:06.732873Z",
     "shell.execute_reply.started": "2023-01-26T03:21:06.721367Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "defaults :\n",
    "  - modulus_default\n",
    "  - /arch/conv_fully_connected_cfg@arch.decoder\n",
    "  - /arch/fno_cfg@arch.fno\n",
    "  - scheduler: tf_exponential_lr\n",
    "  - optimizer: adam\n",
    "  - loss: sum\n",
    "  - _self_\n",
    "\n",
    "arch:\n",
    "  decoder:\n",
    "    input_keys: [z, 32]\n",
    "    output_keys: sol\n",
    "    nr_layers: 1\n",
    "    layer_size: 32\n",
    "\n",
    "  fno:\n",
    "    input_keys: coeff\n",
    "    dimension: 2\n",
    "    nr_fno_layers: 4\n",
    "    fno_modes: 12\n",
    "    padding: 9\n",
    "\n",
    "scheduler:\n",
    "  decay_rate: 0.95\n",
    "  decay_steps: 1000\n",
    "\n",
    "training:\n",
    "  rec_results_freq : 1000\n",
    "  max_steps : 10000\n",
    "\n",
    "batch_size:\n",
    "  grid: 32\n",
    "  validation: 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa46a6-e84a-475c-a68b-ba58135cc55d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0123240d-382f-4848-bbd1-83b804e4d626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-26T03:21:06.735613Z",
     "iopub.status.busy": "2023-01-26T03:21:06.735304Z",
     "iopub.status.idle": "2023-01-26T03:21:06.741978Z",
     "shell.execute_reply": "2023-01-26T03:21:06.741300Z",
     "shell.execute_reply.started": "2023-01-26T03:21:06.735586Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fhnatvdd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fhnatvdd.py\n",
    "\n",
    "import modulus\n",
    "from modulus.hydra import instantiate_arch, ModulusConfig\n",
    "from modulus.key import Key\n",
    "\n",
    "from modulus.solver import Solver\n",
    "from modulus.domain import Domain\n",
    "from modulus.domain.constraint import SupervisedGridConstraint\n",
    "from modulus.domain.validator import GridValidator\n",
    "from modulus.dataset import DictGridDataset\n",
    "from modulus.utils.io.plotter import GridValidatorPlotter\n",
    "from modulus.hydra import to_absolute_path\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "try:\n",
    "    import gdown\n",
    "except:\n",
    "    gdown = None\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "\n",
    "_FNO_datatsets_ids = {\n",
    "    \"Darcy_241\": \"1ViDqN7nc_VCnMackiXv_d7CHZANAFKzV\",\n",
    "    \"Darcy_421\": \"1Z1uxG9R8AdAGJprG5STcphysjm56_0Jf\",\n",
    "}\n",
    "_FNO_dataset_names = {\n",
    "    \"Darcy_241\": (\n",
    "        \"piececonst_r241_N1024_smooth1.hdf5\",\n",
    "        \"piececonst_r241_N1024_smooth2.hdf5\",\n",
    "    ),\n",
    "    \"Darcy_421\": (\n",
    "        \"piececonst_r421_N1024_smooth1.hdf5\",\n",
    "        \"piececonst_r421_N1024_smooth2.hdf5\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def load_FNO_dataset(path, input_keys, output_keys, n_examples=None):\n",
    "    \"Loads a FNO dataset\"\n",
    "\n",
    "    if not path.endswith(\".hdf5\"):\n",
    "        raise Exception(\n",
    "            \".hdf5 file required: please use utilities.preprocess_FNO_mat to convert .mat file\"\n",
    "        )\n",
    "\n",
    "    # load data\n",
    "    path = to_absolute_path(path)\n",
    "    data = h5py.File(path, \"r\")\n",
    "    _ks = [k for k in data.keys() if not k.startswith(\"__\")]\n",
    "    print(f\"loaded: {path}\\navaliable keys: {_ks}\")\n",
    "\n",
    "    # parse data\n",
    "    invar, outvar = dict(), dict()\n",
    "    for d, keys in [(invar, input_keys), (outvar, output_keys)]:\n",
    "        for k in keys:\n",
    "\n",
    "            # get data\n",
    "            x = data[k]  # N, C, H, W\n",
    "\n",
    "            # cut examples out\n",
    "            if n_examples is not None:\n",
    "                x = x[:n_examples]\n",
    "\n",
    "            # print out normalisation values\n",
    "            print(f\"selected key: {k}, mean: {x.mean():.5e}, std: {x.std():.5e}\")\n",
    "\n",
    "            d[k] = x\n",
    "    del data\n",
    "\n",
    "    return (invar, outvar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download_FNO_dataset(name, outdir=\"datasets/\"):\n",
    "    \"Tries to download FNO dataset from drive\"\n",
    "\n",
    "    if name not in _FNO_datatsets_ids:\n",
    "        raise Exception(\n",
    "            f\"Error: FNO dataset {name} not recognised, select one from {list(_FNO_datatsets_ids.keys())}\"\n",
    "        )\n",
    "\n",
    "    id = _FNO_datatsets_ids[name]\n",
    "    outdir = to_absolute_path(outdir) + \"/\"\n",
    "    namedir = f\"{outdir}{name}/\"\n",
    "\n",
    "    # skip if already exists\n",
    "    exists = True\n",
    "    for file_name in _FNO_dataset_names[name]:\n",
    "        if not os.path.isfile(namedir + file_name):\n",
    "            exists = False\n",
    "            break\n",
    "    if exists:\n",
    "        return\n",
    "    print(f\"FNO dataset {name} not detected, downloading dataset\")\n",
    "\n",
    "    # Make sure we have gdown installed\n",
    "    if gdown is None:\n",
    "        raise ModuleNotFoundError(\"gdown package is required to download the dataset!\")\n",
    "\n",
    "    # get output directory\n",
    "    os.makedirs(namedir, exist_ok=True)\n",
    "\n",
    "    # download dataset\n",
    "    zippath = f\"{outdir}{name}.zip\"\n",
    "    _download_file_from_google_drive(id, zippath)\n",
    "\n",
    "    # unzip\n",
    "    with zipfile.ZipFile(zippath, \"r\") as f:\n",
    "        f.extractall(namedir)\n",
    "    os.remove(zippath)\n",
    "\n",
    "    # preprocess files\n",
    "    for file in os.listdir(namedir):\n",
    "        if file.endswith(\".mat\"):\n",
    "            matpath = f\"{namedir}{file}\"\n",
    "            preprocess_FNO_mat(matpath)\n",
    "            os.remove(matpath)\n",
    "\n",
    "\n",
    "def _download_file_from_google_drive(id, path):\n",
    "    \"Downloads a file from google drive\"\n",
    "\n",
    "    # use gdown library to download file\n",
    "    gdown.download(id=id, output=path)\n",
    "\n",
    "\n",
    "def preprocess_FNO_mat(path):\n",
    "    \"Convert a FNO .mat file to a hdf5 file, adding extra dimension to data arrays\"\n",
    "\n",
    "    assert path.endswith(\".mat\")\n",
    "    data = scipy.io.loadmat(path)\n",
    "    ks = [k for k in data.keys() if not k.startswith(\"__\")]\n",
    "    with h5py.File(path[:-4] + \".hdf5\", \"w\") as f:\n",
    "        for k in ks:\n",
    "            x = np.expand_dims(data[k], axis=1)  # N, C, H, W\n",
    "            f.create_dataset(\n",
    "                k, data=x, dtype=\"float32\"\n",
    "            )  # note h5 files larger than .mat because no compression used\n",
    "\n",
    "\n",
    "@modulus.main(config_path=\"./\", config_name=\"config\")\n",
    "def run(cfg: ModulusConfig) -> None:\n",
    "\n",
    "    # load training/ test data\n",
    "    input_keys = [Key(\"coeff\", scale=(7.48360e00, 4.49996e00))]\n",
    "    output_keys = [Key(\"sol\", scale=(5.74634e-03, 3.88433e-03))]\n",
    "\n",
    "    download_FNO_dataset(\"Darcy_241\", outdir=\"datasets/\")\n",
    "    invar_train, outvar_train = load_FNO_dataset(\n",
    "        \"datasets/Darcy_241/piececonst_r241_N1024_smooth1.hdf5\",\n",
    "        [k.name for k in input_keys],\n",
    "        [k.name for k in output_keys],\n",
    "        n_examples=1000,\n",
    "    )\n",
    "    invar_test, outvar_test = load_FNO_dataset(\n",
    "        \"datasets/Darcy_241/piececonst_r241_N1024_smooth2.hdf5\",\n",
    "        [k.name for k in input_keys],\n",
    "        [k.name for k in output_keys],\n",
    "        n_examples=100,\n",
    "    )\n",
    "\n",
    "    # make datasets\n",
    "    train_dataset = DictGridDataset(invar_train, outvar_train)\n",
    "    test_dataset = DictGridDataset(invar_test, outvar_test)\n",
    "\n",
    "    # print out training/ test data shapes\n",
    "    for d in (invar_train, outvar_train, invar_test, outvar_test):\n",
    "        for k in d:\n",
    "            print(f\"{k}: {d[k].shape}\")\n",
    "\n",
    "    decoder_net = instantiate_arch(\n",
    "        cfg=cfg.arch.decoder,\n",
    "        output_keys=output_keys,\n",
    "    )\n",
    "    fno = instantiate_arch(\n",
    "        cfg=cfg.arch.fno,\n",
    "        input_keys=input_keys,\n",
    "        decoder_net=decoder_net,\n",
    "    )\n",
    "    nodes = [fno.make_node('fno')]\n",
    "\n",
    "\n",
    "    # make domain\n",
    "    domain = Domain()\n",
    "\n",
    "    # add constraints to domain\n",
    "    supervised = SupervisedGridConstraint(\n",
    "        nodes=nodes,\n",
    "        dataset=train_dataset,\n",
    "        batch_size=cfg.batch_size.grid,\n",
    "    )\n",
    "    domain.add_constraint(supervised, \"supervised\")\n",
    "\n",
    "    print(train_dataset)\n",
    "    # add validator\n",
    "    val = GridValidator(\n",
    "        nodes,\n",
    "        dataset=test_dataset,\n",
    "        batch_size=cfg.batch_size.validation,\n",
    "        plotter=GridValidatorPlotter(n_examples=5),\n",
    "    )\n",
    "    domain.add_validator(val, \"test\")\n",
    "\n",
    "    # make solver\n",
    "    slv = Solver(cfg, domain)\n",
    "\n",
    "    # start solver\n",
    "    slv.solve()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c040c0-2585-4135-a686-b9719ab67fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-26T03:21:06.743082Z",
     "iopub.status.busy": "2023-01-26T03:21:06.742835Z",
     "iopub.status.idle": "2023-01-26T03:29:02.642447Z",
     "shell.execute_reply": "2023-01-26T03:29:02.641785Z",
     "shell.execute_reply.started": "2023-01-26T03:21:06.743063Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:21:12] - JIT using the NVFuser TorchScript backend\n",
      "[03:21:12] - JitManager: {'_enabled': True, '_arch_mode': <JitArchMode.ONLY_ACTIVATION: 1>, '_use_nvfuser': True, '_autograd_nodes': False}\n",
      "[03:21:12] - GraphManager: {'_func_arch': False, '_debug': False, '_func_arch_allow_partial_hessian': True}\n",
      "loaded: /home/jovyan/t/datasets/Darcy_241/piececonst_r241_N1024_smooth1.hdf5\n",
      "avaliable keys: ['Kcoeff', 'Kcoeff_x', 'Kcoeff_y', 'coeff', 'sol']\n",
      "selected key: coeff, mean: 7.48360e+00, std: 4.49996e+00\n",
      "selected key: sol, mean: 5.74634e-03, std: 3.88433e-03\n",
      "loaded: /home/jovyan/t/datasets/Darcy_241/piececonst_r241_N1024_smooth2.hdf5\n",
      "avaliable keys: ['Kcoeff', 'Kcoeff_x', 'Kcoeff_y', 'coeff', 'sol']\n",
      "selected key: coeff, mean: 7.60960e+00, std: 4.49867e+00\n",
      "selected key: sol, mean: 5.62588e-03, std: 3.80937e-03\n",
      "coeff: (1000, 1, 241, 241)\n",
      "sol: (1000, 1, 241, 241)\n",
      "coeff: (100, 1, 241, 241)\n",
      "sol: (100, 1, 241, 241)\n",
      "<modulus.dataset.discrete.DictGridDataset object at 0x7f3950280940>\n",
      "[03:21:14] - Installed PyTorch version 1.13.1+cu117 is not TorchScript supported in Modulus. Version 1.13.0a0+d321be6 is officially supported.\n",
      "[03:21:14] - attempting to restore from: outputs/fhnatvdd\n",
      "[03:21:14] - optimizer checkpoint not found\n",
      "[03:21:14] - model fno.0.pth not found\n",
      "[03:21:17] - [step:          0] record constraint batch time:  1.757e-01s\n",
      "[03:21:23] - [step:          0] record validators time:  6.181e+00s\n",
      "[03:21:23] - [step:          0] saved checkpoint to outputs/fhnatvdd\n",
      "[03:21:23] - [step:          0] loss:  2.831e+01\n",
      "[03:21:32] - Attempting cuda graph building, this may take a bit...\n",
      "[03:22:08] - [step:        100] loss:  4.099e-01, time/iteration:  4.537e+02 ms\n",
      "[03:22:54] - [step:        200] loss:  1.129e-01, time/iteration:  4.527e+02 ms\n",
      "[03:23:39] - [step:        300] loss:  1.288e-01, time/iteration:  4.519e+02 ms\n",
      "[03:24:24] - [step:        400] loss:  6.156e-02, time/iteration:  4.517e+02 ms\n",
      "[03:25:09] - [step:        500] loss:  5.405e-02, time/iteration:  4.513e+02 ms\n",
      "[03:25:54] - [step:        600] loss:  2.127e-02, time/iteration:  4.510e+02 ms\n",
      "[03:26:39] - [step:        700] loss:  1.305e-02, time/iteration:  4.511e+02 ms\n",
      "[03:27:24] - [step:        800] loss:  1.410e-02, time/iteration:  4.509e+02 ms\n",
      "[03:28:09] - [step:        900] loss:  8.629e-03, time/iteration:  4.511e+02 ms\n",
      "[03:28:55] - [step:       1000] record constraint batch time:  1.607e-01s\n",
      "[03:29:01] - [step:       1000] record validators time:  6.113e+00s\n",
      "[03:29:01] - [step:       1000] saved checkpoint to outputs/fhnatvdd\n",
      "[03:29:01] - [step:       1000] loss:  9.319e-03, time/iteration:  5.149e+02 ms\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!rm -r outputs/fhnatvdd || true ##se não limpar o output ele aproveita o treinamento, mesmo se mudar o modelo\n",
    "!python fhnatvdd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab5b4c-2930-4c71-8c1f-83b019197e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-26T03:29:02.643746Z",
     "iopub.status.busy": "2023-01-26T03:29:02.643431Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'outputs/fhnatv/validators/validator.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/fhnatv/validators/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# plot in 1d\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(base_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidator.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(data\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39marr_0)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue_x1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue x1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/numpy/lib/npyio.py:417\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    418\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'outputs/fhnatv/validators/validator.npz'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_dir = \"outputs/fhnatv/validators/\"\n",
    "\n",
    "# plot in 1d\n",
    "data = np.load(base_dir + \"validator.npz\", allow_pickle=True)\n",
    "data = np.atleast_1d(data.f.arr_0)[0]\n",
    "\n",
    "plt.plot(data[\"t\"], data[\"true_x1\"],\"o\", label=\"True x1\")\n",
    "\n",
    "plt.plot(data[\"t\"], data[\"pred_x1\"], label=\"Pred x1\")\n",
    "\n",
    "plt.ylim(0,2)\n",
    "plt.legend()\n",
    "plt.savefig(\"comparison.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a4164-bbf2-4226-b9a8-9b4a47e67cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263817ff-d5ff-41cf-9d7a-7df866f6dcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
